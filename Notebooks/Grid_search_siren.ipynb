{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grid_search_siren.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6Zfn2m8QCTUO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAcixx9Z3XYH"
      },
      "source": [
        "# Import et préparation de l'environnement\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJOg9MY7khd",
        "trusted": true
      },
      "source": [
        "import shutil\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import imageio\n",
        "import subprocess\n",
        "from IPython import display\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b4_ZrdU_FdV"
      },
      "source": [
        "## Redémarrer l'environnement d'execution du notebook après avoir executer la cellule suivante pour prendre en compte les changements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftkN5c3f_MuZ"
      },
      "source": [
        "Réinstallation de la version de PyTorch pour matcher la version de CUDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf"
      },
      "source": [
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-irecVJr_Zc9"
      },
      "source": [
        "Récupération du package CLIP pour télécharger les modèles pré-entrainés.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm3_VmxpAiB1"
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "\n",
        "%cd /content/CLIP/\n",
        "\n",
        "!pip install ftfy\n",
        "\n",
        "import clip\n",
        "\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26yrBKOTqeVY"
      },
      "source": [
        "Connexion à Drive pour sauvegarder et charger les archives de logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGfC0ZEdYEY_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5odD-rH8t6oz"
      },
      "source": [
        "# Implem de SIREN et du modèle d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCh2D8Dt8Xd"
      },
      "source": [
        "class SineLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        \n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features, \n",
        "                                             1 / self.in_features)      \n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "    \n",
        "    def forward_with_intermediate(self, input):\n",
        "        intermediate = self.omega_0 * self.linear(input)\n",
        "        return torch.sin(intermediate), intermediate\n",
        "    \n",
        "    \n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, img_side_size, hidden_layers, neurons_par_layer, outermost_linear=True, \n",
        "                 first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_size = img_side_size\n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(2, neurons_par_layer, \n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(neurons_par_layer, neurons_par_layer, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(neurons_par_layer, 3)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / neurons_par_layer) / hidden_omega_0, \n",
        "                                              np.sqrt(6 / neurons_par_layer) / hidden_omega_0)\n",
        "                \n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(neurons_par_layer, 3, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "        \n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    \n",
        "    def forward(self, coords):\n",
        "        coords = coords.clone().detach().requires_grad_(True)\n",
        "        output = self.net(coords.cuda())\n",
        "        return output.view(1, self.img_size, self.img_size, 3).permute(0, 3, 1, 2)#.sigmoid_()\n",
        "\n",
        "    def forward_with_activations(self, coords, retain_grad=False):\n",
        "        '''Returns not only model output, but also intermediate activations.\n",
        "        Only used for visualizing activations later!'''\n",
        "        activations = OrderedDict()\n",
        "\n",
        "        activation_count = 0\n",
        "        x = coords.clone().detach().requires_grad_(True)\n",
        "        activations['input'] = x\n",
        "        for i, layer in enumerate(self.net):\n",
        "            if isinstance(layer, SineLayer):\n",
        "                x, intermed = layer.forward_with_intermediate(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    intermed.retain_grad()\n",
        "                    \n",
        "                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n",
        "                activation_count += 1\n",
        "            else: \n",
        "                x = layer(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    \n",
        "            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n",
        "            activation_count += 1\n",
        "\n",
        "        return activations\n",
        "\n",
        "\n",
        "def get_mgrid(sidelen, dim=2):\n",
        "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
        "    sidelen: int\n",
        "    dim: int'''\n",
        "    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
        "    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
        "    mgrid = mgrid.reshape(-1, dim)\n",
        "    return mgrid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EuUz-ICNKUr",
        "cellView": "both",
        "trusted": true
      },
      "source": [
        "\n",
        "class SirenWrapper():\n",
        "  def __init__(self, starting_text, img_size, perceptor, siren, optimizer, lr = 1e-5, log_dir = None):\n",
        "      self.starting_text = starting_text\n",
        "      self.img_size = img_size\n",
        "      self.perceptor = perceptor\n",
        "      self.nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
        "                                                  (0.26862954, 0.26130258, 0.27577711))\n",
        "      self.model = siren\n",
        "      self.optimizer = optimizer(self.model.parameters(), lr)\n",
        "      self.writer = SummaryWriter(log_dir)\n",
        "\n",
        "\n",
        "  def log_metrics(self, loss, epoch, exec_time):\n",
        "    with torch.no_grad():\n",
        "      img = self.nom(self.model(get_mgrid(self.img_size)).cpu()).numpy()[0]\n",
        "    \n",
        "    img = np.array(img)[:,:,:]\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    imageio.imwrite('tmp.png', np.array(img))\n",
        "\n",
        "    self.writer.add_scalar('Loss/train', loss, epoch)\n",
        "    self.writer.add_scalar('Epoch processing time in s', exec_time, epoch)\n",
        "    with PIL.Image.open(\"tmp.png\") as tmp_img:\n",
        "      self.writer.add_image(\"Generated image\", np.array(tmp_img), epoch, dataformats='HWC')\n",
        "    \n",
        "\n",
        "  def ascend_txt(self):\n",
        "    out = self.model(get_mgrid(self.img_size))\n",
        "\n",
        "    cutn = 64\n",
        "    p_s = []\n",
        "    for ch in range(cutn):\n",
        "        size = torch.randint(int(.5 * self.img_size), int(.98 * self.img_size), ())\n",
        "        offsetx = torch.randint(0, self.img_size - size, ())\n",
        "        offsety = torch.randint(0, self.img_size - size, ())\n",
        "        apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n",
        "        apper = torch.nn.functional.interpolate(apper, (224,224), mode='bilinear')\n",
        "        p_s.append(self.nom(apper))\n",
        "    into = torch.cat(p_s, 0)\n",
        "\n",
        "    iii = self.perceptor.encode_image(into)\n",
        "    t = self.perceptor.encode_text(self.starting_text.cuda())\n",
        "    return torch.cosine_similarity(t, iii, dim=-1).mean() * -100\n",
        "\n",
        "\n",
        "  def train_iter(self, epoch):\n",
        "      loss = self.ascend_txt()\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def train(self, epochs, iter_per_epoch):\n",
        "      for epoch in range(epochs):\n",
        "          start_time = time.time()\n",
        "          for _ in tqdm(range(iter_per_epoch)):\n",
        "              loss = self.train_iter(epoch)\n",
        "\n",
        "          self.log_metrics(loss, epoch, time.time() - start_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WztSrRF23Rqg"
      },
      "source": [
        "# Entraînement et grid-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbeZVdA_AGK7"
      },
      "source": [
        "Chaque entrainement se fait sur 500 itérations au total (découpées en 10 epochs de 50 iter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSbpQqzIM03c"
      },
      "source": [
        "# Grid search values\n",
        "txt_list = [\n",
        "            \"A knight fights a red dragon in a volcano.\",\n",
        "            \"An ogre takes a mud bath in a swamp.\",\n",
        "            \"Two birds make a nest in a tall maple.\",\n",
        "            \"It all starts with a mouse searching for food.\"\n",
        "]\n",
        "\n",
        "clip_model_list = [\n",
        "  \"RN50\",\n",
        "  \"RN101\",\n",
        "  \"ViT-B/32\"\n",
        "]\n",
        "\n",
        "hidden_layers_list = [12, 16, 20]\n",
        "\n",
        "neurons_per_layer_list = [128, 256, 512]\n",
        "\n",
        "learning_rate_list = [1e-4, 1e-5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul7AusG9HI6p"
      },
      "source": [
        "for txt_idx, starting_text in enumerate(txt_list):\n",
        "  txt = clip.tokenize(starting_text) \n",
        "\n",
        "  for perc_idx, perceptor_name in enumerate(clip_model_list):\n",
        "    perceptor, _ = clip.load(perceptor_name)\n",
        "\n",
        "    img_side_size = 128\n",
        "    for hl_idx, hidden_layers in enumerate(hidden_layers_list):\n",
        "      for npl_idx, neurons_per_layer in enumerate(neurons_per_layer_list):\n",
        "        siren_model = Siren(img_side_size, hidden_layers, neurons_per_layer).cuda()\n",
        "        \n",
        "        for lr_idx, learning_rate in enumerate(learning_rate_list):\n",
        "          log_dir = f\"runs/{txt_idx}-{perc_idx}-{hl_idx}-{npl_idx}-{lr_idx}\"\n",
        "          optimizer = torch.optim.Adam\n",
        "          siren = SirenWrapper(txt, img_side_size, perceptor, siren_model, optimizer, learning_rate, log_dir)\n",
        "          siren.train(10, 50)\n",
        "          shutil.make_archive(\"/content/drive/MyDrive/Colab Notebooks/siren_training_logs\", 'zip', \"/content/runs/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSJEEe-8rPzk"
      },
      "source": [
        "# Dezip et visualisation des logs via Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIpfalPpYj9R"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/Colab Notebooks/siren_training_logs.zip\" -d \"/content/logs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLODruiANKlu"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSaFc__FNBxA"
      },
      "source": [
        "%tensorboard --logdir /content/logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}